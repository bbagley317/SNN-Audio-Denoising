{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullbandModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, beta=0.9):\n",
    "        \"\"\"\n",
    "        Fullband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(FullbandModel, self).__init__()\n",
    "        \n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.input_dim = freq_bins * time_bins  # Flattened input feature size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Layers\n",
    "        self.normalization = nn.LayerNorm(self.input_dim)  # Normalize input features\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)          # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = nn.Linear(self.input_dim, self.hidden_dim)  # Linear transformation\n",
    "\n",
    "    def forward(self, x, num_steps=10):\n",
    "        \"\"\"\n",
    "        Forward pass for the FullbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        batch_size, freq_bins, time_bins = x.shape\n",
    "        assert freq_bins == self.freq_bins and time_bins == self.time_bins, \\\n",
    "            \"Input dimensions must match model initialization dimensions.\"\n",
    "\n",
    "        # Flatten the spectrogram into 1D vectors\n",
    "        x = x.view(batch_size, -1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Normalize the input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record spiking activity and membrane potentials\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Linear transformation\n",
    "            cur = self.linear(x)\n",
    "\n",
    "            # Spiking neuron dynamics\n",
    "            spk, mem = self.spikingneuron(cur, mem)\n",
    "\n",
    "            # Record outputs\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack outputs across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubbandModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_steps, beta=0.9):\n",
    "        \"\"\"\n",
    "        Subband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(SubbandModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)  # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = None  # Linear layer to be initialized dynamically\n",
    "\n",
    "    def forward(self, x, num_steps=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the SubbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, subband_dim).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        batch_size, subband_dim = x.shape\n",
    "\n",
    "        # Initialize normalization and linear layers dynamically\n",
    "        if not hasattr(self, 'normalization') or self.normalization is None:\n",
    "            self.normalization = nn.LayerNorm(subband_dim).to(x.device)\n",
    "        if self.linear is None:\n",
    "            self.linear = nn.Linear(subband_dim, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Normalize input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record the outputs\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur = self.linear(x)  # Linear transformation\n",
    "            spk, mem = self.spikingneuron(cur, mem)  # Spiking neuron dynamics\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack the recorded values across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def frequency_partition(spectrogram):\n",
    "#     \"\"\"\n",
    "#     Splits the input spectrogram tensor into 3 sub-tensors of different frequencies.\n",
    "\n",
    "#     Args:\n",
    "#     - spectrogram: Input tensor of shape (128, 1938) (frequency bins, time bins).\n",
    "\n",
    "#     Returns:\n",
    "#     - A tuple containing three sub-tensors:\n",
    "#         - low_freq: Frequencies from [0, 16).\n",
    "#         - mid_freq: Frequencies from [16, 64).\n",
    "#         - high_freq: Frequencies from [64, 128]\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if spectrogram.shape != (128, 860):\n",
    "#         raise ValueError(f\"Input spectrogram must have shape (128, 860) - ({spectrogram.shape})\")\n",
    "\n",
    "#     low_freq = spectrogram[:16, :]\n",
    "#     mid_freq = spectrogram[16:64, :]\n",
    "#     high_freq = spectrogram[64:128, :]\n",
    "\n",
    "#     return low_freq, mid_freq, high_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_partition(spectrogram, num_subbands):\n",
    "    \"\"\"\n",
    "    Splits the input tensor into subbands along the second dimension.\n",
    "    Args:\n",
    "    - spectrogram: Input tensor of shape (batch_size, hidden_dim).\n",
    "    - num_subbands: Number of subbands to split the hidden_dim into.\n",
    "    Returns:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size)\n",
    "    \"\"\"\n",
    "    batch_size, hidden_dim = spectrogram.shape\n",
    "    subband_size = hidden_dim // num_subbands\n",
    "\n",
    "    # Split along the hidden_dim axis\n",
    "    subbands = torch.split(spectrogram, subband_size, dim=1)\n",
    "    return subbands\n",
    "\n",
    "\n",
    "def frequency_reconstruct(subbands):\n",
    "    \"\"\"\n",
    "    Reconstructs the full spectrogram from processed subbands.\n",
    "    Args:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size, time_bins).\n",
    "    Returns:\n",
    "    - reconstructed: Tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "    \"\"\"\n",
    "    # Concatenate the processed subbands along the frequency axis\n",
    "    reconstructed = torch.cat(subbands, dim=1)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFilterLayer(nn.Module):\n",
    "    def __init__(self, in_channels=1, num_filters=64, kernel_size=3, dilation_rates=(1, 2, 4)):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "        in_channels (int): Number of input channels. Default is 1, shouldn't be altered as we only have 1 channel.\n",
    "        num_filters (int): Number of filters for intermediate convolution layers. Default is 64.\n",
    "        kernel_size (int): Size of the convolutional kernel. Default is 3.\n",
    "        dilation_rates (tuple): Dilation rates for stacked dilated convolutions. Default is (1, 2, 4).\n",
    "        \"\"\"\n",
    "        super(DeepFilterLayer, self).__init__()\n",
    "\n",
    "        self.input_conv = nn.Conv2d(\n",
    "            in_channels, num_filters, kernel_size=kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "\n",
    "        # Stacked dilated convolutions for temporal context\n",
    "        self.dilated_convs = nn.ModuleList([\n",
    "            nn.Conv2d(\n",
    "                num_filters, num_filters, kernel_size=kernel_size,\n",
    "                dilation=dilation, padding=((kernel_size - 1) * dilation) // 2\n",
    "            )\n",
    "            for dilation in dilation_rates\n",
    "        ])\n",
    "\n",
    "        # Aggregation convolution to reduce back to single output channel\n",
    "        self.output_conv = nn.Conv2d(\n",
    "            num_filters, in_channels, kernel_size=kernel_size, padding=kernel_size // 2\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the DeepFilterLayer.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, freq_bins, time_bins).\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Filtered output of shape (batch_size, freq_bins, time_bins).\n",
    "        \"\"\"\n",
    "        # Ensure the input has a channel dimension\n",
    "        if x.dim() == 3:  # If input shape is (batch_size, freq_bins, time_bins)\n",
    "            x = x.unsqueeze(1)  # Add a channel dimension: (batch_size, 1, freq_bins, time_bins)\n",
    "\n",
    "        # Initial convolution\n",
    "        x = F.relu(self.input_conv(x))\n",
    "\n",
    "        # Stacked dilated convolutions with residual connections\n",
    "        for dilated_conv in self.dilated_convs:\n",
    "            x = F.relu(dilated_conv(x))\n",
    "\n",
    "        # Output convolution to reduce back to the input channel count\n",
    "        x = self.output_conv(x)\n",
    "\n",
    "        return x.squeeze(1)  # Remove the channel dimension to return (batch_size, freq_bins, time_bins)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta=0.9, num_filters=64):\n",
    "        \"\"\"\n",
    "        Integrated model with FullbandModel, SubbandModels, and per-subband DeepFilteringLayers.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - num_subbands: Number of frequency subbands.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(IntegratedModel, self).__init__()\n",
    "\n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.num_subbands = num_subbands\n",
    "\n",
    "        subband_size = freq_bins // num_subbands\n",
    "\n",
    "        # Fullband model\n",
    "        self.fullband_model = FullbandModel(freq_bins, time_bins, hidden_dim, beta)\n",
    "\n",
    "        # Subband models\n",
    "        self.subband_models = nn.ModuleList([\n",
    "            SubbandModel(hidden_dim, num_steps, beta)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "        # Per-subband Deep Filtering Layers\n",
    "        self.deep_filtering_layers = nn.ModuleList([\n",
    "            DeepFilterLayer(in_channels=1, num_filters=num_filters)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, num_steps, clean_time_bins):\n",
    "        \"\"\"\n",
    "        Forward pass through the IntegratedModel.\n",
    "\n",
    "        Args:\n",
    "            x (Tensor): Input tensor of shape (batch_size, freq_bins, time_bins).\n",
    "            num_steps (int): Number of timesteps for spiking neuron simulation.\n",
    "            clean_time_bins (int): Time bins of the clean tensor for resizing.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Filtered output of shape (batch_size, freq_bins, clean_time_bins).\n",
    "        \"\"\"\n",
    "        # Fullband processing\n",
    "        fullband_output, _ = self.fullband_model(x, num_steps)\n",
    "\n",
    "        # Subband processing\n",
    "        subbands = frequency_partition(fullband_output[-1], self.num_subbands)\n",
    "        subband_outputs = [\n",
    "            self.subband_models[i](subband, num_steps=num_steps)[0][-1]\n",
    "            for i, subband in enumerate(subbands)\n",
    "        ]\n",
    "\n",
    "        # Per-subband deep filtering\n",
    "        filtered_subbands = [\n",
    "            self.deep_filtering_layers[i](subband_output.unsqueeze(1))  # Add channel dim for DeepFilterLayer\n",
    "            for i, subband_output in enumerate(subband_outputs)\n",
    "        ]\n",
    "\n",
    "        # Concatenate filtered subbands along the frequency dimension\n",
    "        concatenated_output = torch.cat(filtered_subbands, dim=2)  # Adjust dim for concatenation (C, Freq, Time)\n",
    "\n",
    "        # Reshape concatenated output to 4D for interpolation\n",
    "        concatenated_output = concatenated_output.permute(0, 2, 1).unsqueeze(1)  # Shape: (batch, channels=1, freq, time)\n",
    "\n",
    "        # Interpolation to match clean time bins\n",
    "        filtered_output = F.interpolate(\n",
    "            concatenated_output, size=(self.freq_bins, clean_time_bins), mode='bilinear', align_corners=True\n",
    "        )\n",
    "\n",
    "        # Remove unnecessary channel dimension\n",
    "        filtered_output = filtered_output.squeeze(1)  # Shape: (batch_size, freq_bins, clean_time_bins)\n",
    "\n",
    "        return filtered_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def si_sdr_loss(clean, enhanced, eps=1e-8):\n",
    "    \"\"\"\n",
    "    Computes the Scale-Invariant Signal-to-Distortion Ratio (SI-SDR) Loss.\n",
    "    Args:\n",
    "    - clean: Ground truth clean signal (batch_size, time_steps).\n",
    "    - enhanced: Enhanced (predicted) signal (batch_size, time_steps).\n",
    "    - eps: Small value to avoid division by zero.\n",
    "    Returns:\n",
    "    - Loss value (negative SI-SDR).\n",
    "    \"\"\"\n",
    "    # Ensure signals have zero mean\n",
    "    clean = clean - torch.mean(clean, dim=-1, keepdim=True)\n",
    "    enhanced = enhanced - torch.mean(enhanced, dim=-1, keepdim=True)\n",
    "\n",
    "    # Compute scaling factor\n",
    "    scale = torch.sum(clean * enhanced, dim=-1, keepdim=True) / (torch.sum(clean**2, dim=-1, keepdim=True) + eps)\n",
    "\n",
    "    # Projection of enhanced signal onto clean signal\n",
    "    projection = scale * clean\n",
    "\n",
    "    # Compute noise (residual)\n",
    "    noise = enhanced - projection\n",
    "\n",
    "    # SI-SDR calculation with stabilization\n",
    "    numerator = torch.sum(projection**2, dim=-1) + eps\n",
    "    denominator = torch.sum(noise**2, dim=-1) + eps\n",
    "    si_sdr = 10 * torch.log10(numerator / denominator)\n",
    "\n",
    "    # Return negative SI-SDR as the loss\n",
    "    return -torch.mean(si_sdr)\n",
    "\n",
    "def composite_loss(filtered_output, clean_tensor, \n",
    "                   filtered_output_complex=None, clean_tensor_complex=None, \n",
    "                   alpha=0.5, beta=0.3, p=1, target_metric=4.5):\n",
    "    \"\"\"\n",
    "    Composite loss function combining SI-SDR, proxy MetricGAN+ generator loss, and frequency loss.\n",
    "    Args:\n",
    "    - filtered_output: Time-domain enhanced signal.\n",
    "    - clean_tensor: Time-domain clean signal.\n",
    "    - filtered_output_complex: Complex spectrogram of enhanced signal.\n",
    "    - clean_tensor_complex: Complex spectrogram of clean signal.\n",
    "    - alpha: Weight for SI-SDR loss.\n",
    "    - beta: Weight for proxy MetricGAN+ generator loss.\n",
    "    - p: Power for dynamic range compression in frequency-domain loss.\n",
    "    - target_metric: Ideal target metric score (e.g., 4.5 for PESQ-like proxy metric).\n",
    "    Returns:\n",
    "    - Combined loss value.\n",
    "    \"\"\"\n",
    "    # SI-SDR Loss\n",
    "    si_sdr = si_sdr_loss(clean_tensor, filtered_output)\n",
    "\n",
    "    # Proxy MetricGAN+ Generator Loss\n",
    "    if filtered_output_complex is not None and clean_tensor_complex is not None:\n",
    "        def spectrogram_metric_loss(clean_spectrogram, enhanced_spectrogram, p=1):\n",
    "            magnitude_loss = torch.mean(torch.abs(torch.abs(clean_spectrogram)**p - torch.abs(enhanced_spectrogram)**p))\n",
    "            phase_loss = torch.mean(torch.abs(torch.angle(clean_spectrogram) - torch.angle(enhanced_spectrogram)))\n",
    "            return magnitude_loss + phase_loss\n",
    "\n",
    "        gen_loss = spectrogram_metric_loss(clean_tensor_complex, filtered_output_complex, p=p)\n",
    "        proxy_metric_loss = (gen_loss - target_metric) ** 2\n",
    "    else:\n",
    "        proxy_metric_loss = 0\n",
    "\n",
    "    # Frequency-Domain Loss\n",
    "    if filtered_output_complex is not None and clean_tensor_complex is not None:\n",
    "        freq_loss = torch.mean(\n",
    "            torch.abs(filtered_output_complex.abs()**p - clean_tensor_complex.abs()**p)\n",
    "        ) + torch.mean(\n",
    "            torch.abs(filtered_output_complex - clean_tensor_complex)\n",
    "        )\n",
    "    else:\n",
    "        freq_loss = 0\n",
    "\n",
    "    # Composite Loss\n",
    "    total_loss = alpha * si_sdr + beta * proxy_metric_loss + freq_loss\n",
    "    return total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resample clean_tensor to match noisy_tensor's time bins\n",
    "def resample_tensor(clean_tensor, target_time_bins):\n",
    "    \"\"\"\n",
    "    Resamples the clean tensor to match the target time bins using interpolation.\n",
    "    Args:\n",
    "        clean_tensor (torch.Tensor): Tensor of shape [batch, freq_bins, time_bins].\n",
    "        target_time_bins (int): Target number of time bins.\n",
    "    Returns:\n",
    "        torch.Tensor: Resampled tensor of shape [batch, freq_bins, target_time_bins].\n",
    "    \"\"\"\n",
    "    batch, freq_bins, time_bins = clean_tensor.shape\n",
    "    clean_tensor_resampled = F.interpolate(\n",
    "        clean_tensor.unsqueeze(1),  # Add a channel dimension\n",
    "        size=(freq_bins, target_time_bins),  # Resample to target size\n",
    "        mode=\"bilinear\",\n",
    "        align_corners=False\n",
    "    ).squeeze(1)  # Remove the added channel dimension\n",
    "    return clean_tensor_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Paths to the feature and label directories\n",
    "feature_dir = \"E:/CS541 - Deep Learning/noisy_audio_np\"\n",
    "label_dir = \"E:/CS541 - Deep Learning/clean_audio_np\"\n",
    "\n",
    "# Load all feature and label file paths\n",
    "feature_files = sorted(os.listdir(feature_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "indices = torch.arange(len(feature_files))\n",
    "\n",
    "# Ensure that there are the same number of clean and noisy files\n",
    "assert len(feature_files) == len(label_files), \"Mismatch between feature and label file counts!\"\n",
    "\n",
    "# Need to split the data into training and testing - Running an 80/20 split here, lacking validation due to time constraints (It is 6:45 on Wednesday Night!)\n",
    "train_features, test_features, train_labels, test_labels, train_indices, test_indices = train_test_split(feature_files, label_files, indices, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tqdm import tqdm  # For tracking progress\n",
    "\n",
    "# # Model parameters\n",
    "# freq_bins, time_bins = 128, 860     # Based on the dimensions of your spectrograms\n",
    "# hidden_dims = [32,64,128,256]       # Hidden dimension for spiking neurons\n",
    "# num_steps_list = [10,15,25,100]     # Number of timesteps for spiking neurons\n",
    "# num_subbands_list = 4               # Number of subbands (adjusted for even division)\n",
    "# betas = [0.1,0.5,0.9]               # Decay parameter for LIF neurons\n",
    "\n",
    "# # Best subset:\n",
    "# # 128, 860\n",
    "# # 32\n",
    "# # 10\n",
    "# # 4\n",
    "# # 0.9\n",
    "\n",
    "# # Initialize the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Track losses\n",
    "# average_losses = []\n",
    "\n",
    "# for hidden_dim in hidden_dims:\n",
    "#     for num_subbands in num_subbands_list:\n",
    "#         for num_steps in num_steps_list:\n",
    "#             for beta in betas:\n",
    "\n",
    "#                 model = IntegratedModel(freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta).to(device)\n",
    "#                 model.train()\n",
    "\n",
    "#                 # Optimizer\n",
    "#                 optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "#                 losses = []\n",
    "\n",
    "#                 # Training loop\n",
    "#                 for idx in tqdm(range(len(train_features)), desc=\"Processing files\"):\n",
    "#                     feature_path = os.path.join(feature_dir, train_features[idx])\n",
    "#                     label_path = os.path.join(label_dir, train_labels[idx])\n",
    "\n",
    "#                     noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0).to(device)\n",
    "#                     clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0).to(device)\n",
    "\n",
    "#                     clean_tensor_resampled = resample_tensor(clean_tensor, noisy_tensor.shape[2])\n",
    "\n",
    "#                     # Forward pass\n",
    "#                     filtered_output = model(noisy_tensor, num_steps, clean_time_bins=clean_tensor.shape[2])\n",
    "\n",
    "#                     # Compute composite loss\n",
    "#                     total_loss = composite_loss(\n",
    "#                         filtered_output, clean_tensor,\n",
    "#                         filtered_output_complex=None, clean_tensor_complex=None,  # Replace None if using spectrograms\n",
    "#                         alpha=0.25, beta=0.1, p=1, target_metric=4.5\n",
    "#                     )\n",
    "                    \n",
    "#                     losses.append(total_loss)\n",
    "\n",
    "#                     # Backpropagation\n",
    "#                     optimizer.zero_grad()\n",
    "#                     total_loss.backward()\n",
    "#                     optimizer.step()\n",
    "\n",
    "#                 # Print average loss across all files\n",
    "#                 average_loss = sum(losses) / len(losses)\n",
    "#                 average_losses.append(average_loss)\n",
    "#                 print(f\"\\nAverage Loss across all files: {average_loss:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [11:06<00:00,  9.61it/s]\n",
      "Processing files: 100%|██████████| 6400/6400 [10:02<00:00, 10.61it/s]\n",
      "Processing files:  14%|█▍        | 887/6400 [01:21<08:29, 10.83it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 48\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n\u001b[0;32m     47\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 48\u001b[0m         \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Print average loss across all files\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jackh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\_tensor.py:581\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    573\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    574\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    579\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    580\u001b[0m     )\n\u001b[1;32m--> 581\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    582\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    583\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jackh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\jackh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    823\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    824\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 825\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    826\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    827\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    828\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    829\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # For tracking progress\n",
    "\n",
    "# Model parameters\n",
    "freq_bins, time_bins = 128, 860  # Based on the dimensions of your spectrograms\n",
    "hidden_dim = 32                  # Hidden dimension for hidden layers\n",
    "num_steps = 10                   # Number of timesteps for spiking neurons\n",
    "num_subbands = 4                 # Number of subbands (adjusted for even division, ideally wouldve been low/medium/high freq. partition)\n",
    "beta = 0.9                       # Decay parameter for LIF neurons\n",
    "\n",
    "epochs = 3                       # Number of epochs to train for\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = IntegratedModel(freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta).to(device)\n",
    "model.train()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "# Track losses\n",
    "losses = []\n",
    "\n",
    "for _ in range(epochs):\n",
    "    # Training loop\n",
    "    for idx in tqdm(range(len(train_features)), desc=\"Processing files\"):\n",
    "        feature_path = os.path.join(feature_dir, train_features[idx])\n",
    "        label_path = os.path.join(label_dir, train_labels[idx])\n",
    "\n",
    "        noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0).to(device)\n",
    "        clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0).to(device)\n",
    "\n",
    "        clean_tensor_resampled = resample_tensor(clean_tensor, noisy_tensor.shape[2])\n",
    "\n",
    "        # Forward pass\n",
    "        filtered_output = model(noisy_tensor, num_steps, clean_time_bins=clean_tensor.shape[2])\n",
    "\n",
    "        # Compute composite loss\n",
    "        total_loss = composite_loss(\n",
    "            filtered_output, clean_tensor,\n",
    "            filtered_output_complex=None, clean_tensor_complex=None,  # Replace None if using spectrograms\n",
    "            alpha=0.25, beta=0.1, p=1, target_metric=4.5\n",
    "        )\n",
    "        \n",
    "        losses.append(total_loss)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Print average loss across all files\n",
    "average_loss = sum(losses) / len(losses)\n",
    "print(f\"\\nAverage Loss across all files: {average_loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [10:03<00:00, 10.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [14:55<00:00,  7.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [10:05<00:00, 10.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [15:17<00:00,  6.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [10:33<00:00, 10.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 6400/6400 [16:26<00:00,  6.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Average Loss across all files: 0.000000\n"
     ]
    }
   ],
   "source": [
    "# from tqdm import tqdm  # For tracking progress\n",
    "\n",
    "# # Model parameters\n",
    "# freq_bins, time_bins = 128, 860     # Based on the dimensions of your spectrograms\n",
    "# hidden_dims = [32,64,128,256]       # Hidden dimension for spiking neurons\n",
    "# num_steps_list = [10,15,25,100]     # Number of timesteps for spiking neurons\n",
    "# num_subbands      # Number of subbands\n",
    "# beta = 0.9                          # Decay parameter for LIF neurons\n",
    "\n",
    "# # Best subset:\n",
    "# # 128, 860\n",
    "# # 32\n",
    "# # 10\n",
    "# # 4\n",
    "# # 0.9\n",
    "\n",
    "# # Initialize the model\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Track losses\n",
    "# average_losses = []\n",
    "\n",
    "# for hidden_dim in hidden_dims:\n",
    "#     for num_subbands in num_subbands_list:\n",
    "#         for num_steps in num_steps_list:\n",
    "\n",
    "#             model = IntegratedModel(freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta).to(device)\n",
    "#             model.train()\n",
    "\n",
    "#             # Optimizer\n",
    "#             optimizer = torch.optim.AdamW(model.parameters(), lr=0.001)\n",
    "\n",
    "#             losses = []\n",
    "\n",
    "#             # Training loop\n",
    "#             for idx in tqdm(range(len(train_features)), desc=\"Processing files\"):\n",
    "#                 feature_path = os.path.join(feature_dir, train_features[idx])\n",
    "#                 label_path = os.path.join(label_dir, train_labels[idx])\n",
    "\n",
    "#                 noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0).to(device)\n",
    "#                 clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0).to(device)\n",
    "\n",
    "#                 clean_tensor_resampled = resample_tensor(clean_tensor, noisy_tensor.shape[2])\n",
    "\n",
    "#                 # Forward pass\n",
    "#                 filtered_output = model(noisy_tensor, num_steps, clean_time_bins=clean_tensor.shape[2])\n",
    "\n",
    "#                 # Compute composite loss\n",
    "#                 total_loss = composite_loss(\n",
    "#                     filtered_output, clean_tensor,\n",
    "#                     filtered_output_complex=None, clean_tensor_complex=None,  # Replace None if using spectrograms\n",
    "#                     alpha=0.25, beta=0.1, p=1, target_metric=4.5\n",
    "#                 )\n",
    "                \n",
    "#                 losses.append(total_loss)\n",
    "\n",
    "#                 # Backpropagation\n",
    "#                 optimizer.zero_grad()\n",
    "#                 total_loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             # Print average loss across all files\n",
    "#             average_loss = sum(losses) / len(losses)\n",
    "#             average_losses.append(average_loss)\n",
    "#             print(f\"\\nAverage Loss across all files: {average_loss:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000000002345\n",
      "0.000000006213\n",
      "0.000000002729\n",
      "0.000000005716\n",
      "0.000000005132\n",
      "0.000000005178\n"
     ]
    }
   ],
   "source": [
    "# for loss in average_losses:\n",
    "#     print(f\"{loss:.12f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "\n",
    "test_loss = 0\n",
    "test_set_size = len(test_features)\n",
    "filtered_spectrograms = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for idx in tqdm(range(len(test_features)), desc=\"Processing files\"):\n",
    "        feature_path = os.path.join(feature_dir, test_features[idx])\n",
    "        label_path = os.path.join(label_dir, test_labels[idx])\n",
    "\n",
    "        noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0).to(device)\n",
    "        clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0).to(device)\n",
    "\n",
    "        clean_tensor_resampled = resample_tensor(clean_tensor, noisy_tensor.shape[2])\n",
    "\n",
    "        # Forward pass\n",
    "        filtered_output = model(noisy_tensor, num_steps=10, clean_time_bins=clean_tensor.shape[2])\n",
    "        filtered_spectrograms.append(filtered_output)\n",
    "\n",
    "        # Compute composite loss\n",
    "        loss = composite_loss(\n",
    "            filtered_output, clean_tensor,\n",
    "            filtered_output_complex=None, clean_tensor_complex=None,  # Replace None if using spectrograms\n",
    "            alpha=0.25, beta=0.1, p=1, target_metric=4.5\n",
    "        )\n",
    "        test_loss += loss\n",
    "\n",
    "# Calculate average test loss\n",
    "avg_test_loss = test_loss / test_set_size\n",
    "print(f\"Average Test Loss: {avg_test_loss:.8f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
