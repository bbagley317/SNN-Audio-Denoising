{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# self.gated_spiking = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, hidden_dim),\n",
    "        #     nn.Sigmoid()  # Simulates gating\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullbandModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, beta=0.9):\n",
    "        \"\"\"\n",
    "        Fullband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(FullbandModel, self).__init__()\n",
    "        \n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.input_dim = freq_bins * time_bins  # Flattened input feature size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Layers\n",
    "        self.normalization = nn.LayerNorm(self.input_dim)  # Normalize input features\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)          # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = nn.Linear(self.input_dim, self.hidden_dim)  # Linear transformation\n",
    "\n",
    "    def forward(self, x, num_steps=10):\n",
    "        \"\"\"\n",
    "        Forward pass for the FullbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        batch_size, freq_bins, time_bins = x.shape\n",
    "        assert freq_bins == self.freq_bins and time_bins == self.time_bins, \\\n",
    "            \"Input dimensions must match model initialization dimensions.\"\n",
    "\n",
    "        # Flatten the spectrogram into 1D vectors\n",
    "        x = x.view(batch_size, -1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Normalize the input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record spiking activity and membrane potentials\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Linear transformation\n",
    "            cur = self.linear(x)\n",
    "\n",
    "            # Spiking neuron dynamics\n",
    "            spk, mem = self.spikingneuron(cur, mem)\n",
    "\n",
    "            # Record outputs\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack outputs across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubbandModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_steps, beta=0.9):\n",
    "        \"\"\"\n",
    "        Subband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(SubbandModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)  # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = None  # Linear layer to be initialized dynamically\n",
    "\n",
    "    def forward(self, x, num_steps=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the SubbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, subband_dim).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        batch_size, subband_dim = x.shape\n",
    "\n",
    "        # Initialize normalization and linear layers dynamically\n",
    "        if not hasattr(self, 'normalization') or self.normalization is None:\n",
    "            self.normalization = nn.LayerNorm(subband_dim).to(x.device)\n",
    "        if self.linear is None:\n",
    "            self.linear = nn.Linear(subband_dim, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Normalize input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record the outputs\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur = self.linear(x)  # Linear transformation\n",
    "            spk, mem = self.spikingneuron(cur, mem)  # Spiking neuron dynamics\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack the recorded values across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_partition(spectrogram, num_subbands):\n",
    "    \"\"\"\n",
    "    Splits the input tensor into subbands along the second dimension.\n",
    "    Args:\n",
    "    - spectrogram: Input tensor of shape (batch_size, hidden_dim).\n",
    "    - num_subbands: Number of subbands to split the hidden_dim into.\n",
    "    Returns:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size).\n",
    "    \"\"\"\n",
    "    batch_size, hidden_dim = spectrogram.shape\n",
    "    subband_size = hidden_dim // num_subbands\n",
    "\n",
    "    # Split along the hidden_dim axis\n",
    "    subbands = torch.split(spectrogram, subband_size, dim=1)\n",
    "    print(f\"Number of Subbands (fp func): {len(subbands)}\")\n",
    "    return subbands\n",
    "\n",
    "\n",
    "def frequency_reconstruct(subbands):\n",
    "    \"\"\"\n",
    "    Reconstructs the full spectrogram from processed subbands.\n",
    "    Args:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size, time_bins).\n",
    "    Returns:\n",
    "    - reconstructed: Tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "    \"\"\"\n",
    "    # Concatenate the processed subbands along the frequency axis\n",
    "    reconstructed = torch.cat(subbands, dim=1)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class IntegratedModel(nn.Module):\n",
    "#     def __init__(self, freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta=0.9):\n",
    "#         \"\"\"\n",
    "#         Integrated model combining FullbandModel, frequency partitioning, and SubbandModels.\n",
    "#         Args:\n",
    "#         - freq_bins: Number of frequency bins in the spectrogram.\n",
    "#         - time_bins: Number of time bins in the spectrogram.\n",
    "#         - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "#         - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "#         - num_subbands: Number of frequency subbands.\n",
    "#         - beta: Decay parameter for the LIF neuron.\n",
    "#         \"\"\"\n",
    "#         super(IntegratedModel, self).__init__()\n",
    "\n",
    "#         self.freq_bins = freq_bins\n",
    "#         self.time_bins = time_bins\n",
    "#         self.num_subbands = num_subbands\n",
    "\n",
    "#         subband_size = freq_bins // num_subbands\n",
    "\n",
    "#         # Fullband model\n",
    "#         self.fullband_model = FullbandModel(freq_bins, time_bins, hidden_dim, beta)\n",
    "\n",
    "#         # Subband models\n",
    "#         self.subband_models = nn.ModuleList([\n",
    "#             SubbandModel(hidden_dim, num_steps, beta)\n",
    "#             for _ in range(num_subbands)\n",
    "#         ])\n",
    "        \n",
    "#         print(f\"Number of Subband Models: {len(self.subband_models)}\")\n",
    "\n",
    "#     def forward(self, x, num_steps=10):\n",
    "#         \"\"\"\n",
    "#         Forward pass through FullbandModel, frequency partitioning, and SubbandModels.\n",
    "#         Args:\n",
    "#         - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "#         - num_steps: Number of time steps for spiking neuron simulation.\n",
    "#         Returns:\n",
    "#         - subband_outputs: List of tensors, each corresponding to the output of a SubbandModel.\n",
    "#         \"\"\"\n",
    "#         # Fullband processing\n",
    "#         fullband_output, _ = self.fullband_model(x, num_steps)\n",
    "\n",
    "#         # Use the last timestep output for partitioning\n",
    "#         subbands = frequency_partition(fullband_output[-1], self.num_subbands)\n",
    "#         print(f\"Number of Subbands: {len(subbands)}\")\n",
    "\n",
    "#         # Subband processing\n",
    "#         subband_outputs = [\n",
    "#             self.subband_models[i](subband, num_steps=num_steps)[0][-1]\n",
    "#             for i, subband in enumerate(subbands)\n",
    "#         ]\n",
    "\n",
    "#         return subband_outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFilteringLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_filters=64):\n",
    "        \"\"\"\n",
    "        Deep filtering layer inspired by DeepFilterNet.\n",
    "        Args:\n",
    "        - input_dim: Number of input features (e.g., combined subband dimensions).\n",
    "        - output_dim: Number of output features (e.g., original spectrogram dimensions).\n",
    "        - num_filters: Number of filters in the convolutional layers.\n",
    "        \"\"\"\n",
    "        super(DeepFilteringLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(num_filters, output_dim, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the deep filtering layer.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, input_dim, time_steps).\n",
    "        Returns:\n",
    "        - Output tensor of shape (batch_size, output_dim, time_steps).\n",
    "        \"\"\"\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta=0.9):\n",
    "        \"\"\"\n",
    "        Integrated model with FullbandModel, SubbandModels, and per-subband DeepFilteringLayers.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - num_subbands: Number of frequency subbands.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(IntegratedModel, self).__init__()\n",
    "\n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.num_subbands = num_subbands\n",
    "\n",
    "        subband_size = freq_bins // num_subbands\n",
    "\n",
    "        # Fullband model\n",
    "        self.fullband_model = FullbandModel(freq_bins, time_bins, hidden_dim, beta)\n",
    "\n",
    "        # Subband models\n",
    "        self.subband_models = nn.ModuleList([\n",
    "            SubbandModel(hidden_dim, num_steps, beta)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "        # Per-subband Deep Filtering Layers\n",
    "        self.deep_filtering_layers = nn.ModuleList([\n",
    "            DeepFilteringLayer(input_dim=hidden_dim, output_dim=subband_size)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, num_steps=10):\n",
    "        \"\"\"\n",
    "        Forward pass through the IntegratedModel.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - concatenated_output: Tensor after deep filtering and concatenation.\n",
    "        \"\"\"\n",
    "        # Fullband processing\n",
    "        fullband_output, _ = self.fullband_model(x, num_steps)\n",
    "\n",
    "        # Subband processing\n",
    "        subbands = frequency_partition(fullband_output[-1], self.num_subbands)\n",
    "        subband_outputs = [\n",
    "            self.subband_models[i](subband, num_steps=num_steps)[0][-1]\n",
    "            for i, subband in enumerate(subbands)\n",
    "        ]\n",
    "\n",
    "        # Per-subband deep filtering\n",
    "        filtered_subbands = [\n",
    "            self.deep_filtering_layers[i](subband_output.unsqueeze(-1)).squeeze(-1)\n",
    "            for i, subband_output in enumerate(subband_outputs)\n",
    "        ]\n",
    "\n",
    "        # Concatenate filtered subbands along the feature dimension\n",
    "        concatenated_output = torch.cat(filtered_subbands, dim=1)  # Shape: (batch_size, freq_bins)\n",
    "\n",
    "        return concatenated_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded noisy tensor shape: torch.Size([1, 128, 860])\n",
      "Loaded clean tensor shape: torch.Size([1, 128, 1938])\n",
      "Number of Subbands (fp func): 4\n",
      "Filtered output shape: torch.Size([1, 128])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Paths to the feature and label directories\n",
    "feature_dir = \"E:/CS541 - Deep Learning/noisy_audio_np\"\n",
    "label_dir = \"E:/CS541 - Deep Learning/clean_audio_np\"\n",
    "\n",
    "# Load a sample feature and label pair\n",
    "feature_files = sorted(os.listdir(feature_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "# Select an example\n",
    "idx = 0  # Change this index to test different samples\n",
    "feature_path = os.path.join(feature_dir, feature_files[0])\n",
    "label_path = os.path.join(label_dir, label_files[0])\n",
    "\n",
    "# Load the .npy files\n",
    "noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0)  # Shape: (1, freq_bins, time_bins)\n",
    "clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0)  # Shape: (1, freq_bins, time_bins)\n",
    "\n",
    "# Print shapes to verify data loading\n",
    "print(f\"Loaded noisy tensor shape: {noisy_tensor.shape}\")\n",
    "print(f\"Loaded clean tensor shape: {clean_tensor.shape}\")\n",
    "\n",
    "# Move tensors to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "noisy_tensor = noisy_tensor.to(device)\n",
    "clean_tensor = clean_tensor.to(device)\n",
    "\n",
    "# Model parameters\n",
    "freq_bins, time_bins = noisy_tensor.shape[1], noisy_tensor.shape[2]\n",
    "hidden_dim = 64        # Hidden dimension for spiking neurons\n",
    "num_steps = 10         # Number of timesteps for spiking neurons\n",
    "num_subbands = 4       # Number of subbands (adjusted for even division)\n",
    "beta = 0.9             # Decay parameter for LIF neurons\n",
    "\n",
    "# Initialize the model\n",
    "model = IntegratedModel(freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta).to(device)\n",
    "\n",
    "# Forward pass through the model\n",
    "with torch.no_grad():  # Disable gradients for testing\n",
    "    filtered_output = model(noisy_tensor, num_steps=num_steps)\n",
    "\n",
    "# Print the output shape\n",
    "print(f\"Filtered output shape: {filtered_output.shape}\")\n",
    "\n",
    "# Optional: Compare the filtered output with the clean tensor\n",
    "# loss = torch.nn.MSELoss()(filtered_output, clean_tensor)\n",
    "# print(f\"Loss (MSE) between filtered output and clean tensor: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
