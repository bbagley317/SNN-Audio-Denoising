{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import snntorch as snn\n",
    "from snntorch import spikeplot as splt\n",
    "from snntorch import spikegen\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import itertools\n",
    "\n",
    "# self.gated_spiking = nn.Sequential(\n",
    "        #     nn.Linear(input_dim, hidden_dim),\n",
    "        #     nn.Sigmoid()  # Simulates gating\n",
    "        # )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullbandModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, beta=0.9):\n",
    "        \"\"\"\n",
    "        Fullband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(FullbandModel, self).__init__()\n",
    "        \n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.input_dim = freq_bins * time_bins  # Flattened input feature size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Layers\n",
    "        self.normalization = nn.LayerNorm(self.input_dim)  # Normalize input features\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)          # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = nn.Linear(self.input_dim, self.hidden_dim)  # Linear transformation\n",
    "\n",
    "    def forward(self, x, num_steps=10):\n",
    "        \"\"\"\n",
    "        Forward pass for the FullbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        batch_size, freq_bins, time_bins = x.shape\n",
    "        assert freq_bins == self.freq_bins and time_bins == self.time_bins, \\\n",
    "            \"Input dimensions must match model initialization dimensions.\"\n",
    "\n",
    "        # Flatten the spectrogram into 1D vectors\n",
    "        x = x.view(batch_size, -1)  # Shape: (batch_size, input_dim)\n",
    "\n",
    "        # Normalize the input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record spiking activity and membrane potentials\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            # Linear transformation\n",
    "            cur = self.linear(x)\n",
    "\n",
    "            # Spiking neuron dynamics\n",
    "            spk, mem = self.spikingneuron(cur, mem)\n",
    "\n",
    "            # Record outputs\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack outputs across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubbandModel(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_steps, beta=0.9):\n",
    "        \"\"\"\n",
    "        Subband Model with normalization, spiking neuron layer, and a linear layer.\n",
    "        Args:\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(SubbandModel, self).__init__()\n",
    "\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_steps = num_steps\n",
    "        self.spikingneuron = snn.Leaky(beta=beta)  # Leaky Integrate-and-Fire neuron\n",
    "        self.linear = None  # Linear layer to be initialized dynamically\n",
    "\n",
    "    def forward(self, x, num_steps=None):\n",
    "        \"\"\"\n",
    "        Forward pass for the SubbandModel with time-stepped spiking neuron dynamics.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, subband_dim).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        Returns:\n",
    "        - spk_rec: Spiking activity across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        - mem_rec: Membrane potential across timesteps (num_steps, batch_size, hidden_dim).\n",
    "        \"\"\"\n",
    "        if num_steps is None:\n",
    "            num_steps = self.num_steps\n",
    "\n",
    "        batch_size, subband_dim = x.shape\n",
    "\n",
    "        # Initialize normalization and linear layers dynamically\n",
    "        if not hasattr(self, 'normalization') or self.normalization is None:\n",
    "            self.normalization = nn.LayerNorm(subband_dim).to(x.device)\n",
    "        if self.linear is None:\n",
    "            self.linear = nn.Linear(subband_dim, self.hidden_dim).to(x.device)\n",
    "\n",
    "        # Normalize input\n",
    "        x = self.normalization(x)\n",
    "\n",
    "        # Initialize membrane potentials for the spiking neuron\n",
    "        mem = torch.zeros((batch_size, self.hidden_dim), dtype=torch.float32, device=x.device)\n",
    "\n",
    "        # Record the outputs\n",
    "        spk_rec = []\n",
    "        mem_rec = []\n",
    "\n",
    "        for step in range(num_steps):\n",
    "            cur = self.linear(x)  # Linear transformation\n",
    "            spk, mem = self.spikingneuron(cur, mem)  # Spiking neuron dynamics\n",
    "            spk_rec.append(spk)\n",
    "            mem_rec.append(mem)\n",
    "\n",
    "        # Stack the recorded values across timesteps\n",
    "        spk_rec = torch.stack(spk_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "        mem_rec = torch.stack(mem_rec, dim=0)  # Shape: (num_steps, batch_size, hidden_dim)\n",
    "\n",
    "        return spk_rec, mem_rec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frequency_partition(spectrogram, num_subbands):\n",
    "    \"\"\"\n",
    "    Splits the input tensor into subbands along the second dimension.\n",
    "    Args:\n",
    "    - spectrogram: Input tensor of shape (batch_size, hidden_dim).\n",
    "    - num_subbands: Number of subbands to split the hidden_dim into.\n",
    "    Returns:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size).\n",
    "    \"\"\"\n",
    "    batch_size, hidden_dim = spectrogram.shape\n",
    "    subband_size = hidden_dim // num_subbands\n",
    "\n",
    "    # Split along the hidden_dim axis\n",
    "    subbands = torch.split(spectrogram, subband_size, dim=1)\n",
    "    # print(f\"Number of Subbands (fp func): {len(subbands)}\")\n",
    "    return subbands\n",
    "\n",
    "\n",
    "def frequency_reconstruct(subbands):\n",
    "    \"\"\"\n",
    "    Reconstructs the full spectrogram from processed subbands.\n",
    "    Args:\n",
    "    - subbands: List of tensors, each of shape (batch_size, subband_size, time_bins).\n",
    "    Returns:\n",
    "    - reconstructed: Tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "    \"\"\"\n",
    "    # Concatenate the processed subbands along the frequency axis\n",
    "    reconstructed = torch.cat(subbands, dim=1)\n",
    "    return reconstructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepFilteringLayer(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, num_filters=64):\n",
    "        super(DeepFilteringLayer, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(input_dim, num_filters, kernel_size=3, padding=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv1d(num_filters, num_filters, kernel_size=3, padding=1)\n",
    "        self.conv3 = nn.Conv1d(num_filters, output_dim, kernel_size=1)\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='linear', align_corners=True)  # Add this layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.upsample(x)  # Upsample to match original dimensions\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "class IntegratedModel(nn.Module):\n",
    "    def __init__(self, freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta=0.9):\n",
    "        \"\"\"\n",
    "        Integrated model with FullbandModel, SubbandModels, and per-subband DeepFilteringLayers.\n",
    "        Args:\n",
    "        - freq_bins: Number of frequency bins in the spectrogram.\n",
    "        - time_bins: Number of time bins in the spectrogram.\n",
    "        - hidden_dim: Number of hidden units for the spiking neuron layer.\n",
    "        - num_steps: Number of timesteps for spiking neuron simulation.\n",
    "        - num_subbands: Number of frequency subbands.\n",
    "        - beta: Decay parameter for the LIF neuron.\n",
    "        \"\"\"\n",
    "        super(IntegratedModel, self).__init__()\n",
    "\n",
    "        self.freq_bins = freq_bins\n",
    "        self.time_bins = time_bins\n",
    "        self.num_subbands = num_subbands\n",
    "\n",
    "        subband_size = freq_bins // num_subbands\n",
    "\n",
    "        # Fullband model\n",
    "        self.fullband_model = FullbandModel(freq_bins, time_bins, hidden_dim, beta)\n",
    "\n",
    "        # Subband models\n",
    "        self.subband_models = nn.ModuleList([\n",
    "            SubbandModel(hidden_dim, num_steps, beta)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "        # Per-subband Deep Filtering Layers\n",
    "        self.deep_filtering_layers = nn.ModuleList([\n",
    "            DeepFilteringLayer(input_dim=hidden_dim, output_dim=subband_size)\n",
    "            for _ in range(num_subbands)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, num_steps=10, clean_time_bins=None):\n",
    "        \"\"\"\n",
    "        Forward pass through the IntegratedModel.\n",
    "        Args:\n",
    "        - x: Input tensor of shape (batch_size, frequency_bins, time_bins).\n",
    "        - num_steps: Number of time steps for spiking neuron simulation.\n",
    "        - clean_time_bins: Time bins of the clean tensor for interpolation (optional).\n",
    "        Returns:\n",
    "        - filtered_output: Tensor after deep filtering and interpolation.\n",
    "        \"\"\"\n",
    "        # Fullband processing\n",
    "        fullband_output, _ = self.fullband_model(x, num_steps)\n",
    "\n",
    "        # Subband processing\n",
    "        subbands = frequency_partition(fullband_output[-1], self.num_subbands)\n",
    "        subband_outputs = [\n",
    "            self.subband_models[i](subband, num_steps=num_steps)[0][-1]\n",
    "            for i, subband in enumerate(subbands)\n",
    "        ]\n",
    "\n",
    "        # Per-subband deep filtering\n",
    "        filtered_subbands = [\n",
    "            self.deep_filtering_layers[i](subband_output.unsqueeze(-1)).squeeze(-1)\n",
    "            for i, subband_output in enumerate(subband_outputs)\n",
    "        ]\n",
    "\n",
    "        # Concatenate filtered subbands along the feature dimension\n",
    "        concatenated_output = torch.cat(filtered_subbands, dim=1)  # Shape: (batch_size, freq_bins)\n",
    "\n",
    "        # Add a singleton spatial dimension for interpolation\n",
    "        concatenated_output = concatenated_output.unsqueeze(-1)  # Shape: (batch_size, freq_bins, 1)\n",
    "\n",
    "        # Use clean_time_bins for interpolation if provided\n",
    "        target_time_bins = clean_time_bins if clean_time_bins is not None else self.time_bins\n",
    "\n",
    "        # Interpolate to match the original spectrogram dimensions\n",
    "        interpolated_output = torch.nn.functional.interpolate(\n",
    "            concatenated_output, size=(self.freq_bins, target_time_bins), mode='bilinear', align_corners=True\n",
    "        )\n",
    "\n",
    "        # Remove the singleton spatial dimension\n",
    "        filtered_output = interpolated_output.squeeze(-1)  # Shape: (batch_size, freq_bins, target_time_bins)\n",
    "\n",
    "        return filtered_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:   0%|          | 0/8000 [00:00<?, ?it/s]c:\\Users\\jackh\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\nn\\modules\\loss.py:608: UserWarning: Using a target size (torch.Size([1, 128, 1938])) that is different to the input size (torch.Size([1, 128, 128, 1938])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n",
      "Processing files:   5%|▍         | 379/8000 [00:54<19:29,  6.52it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm  # For tracking progress\n",
    "\n",
    "# Paths to the feature and label directories\n",
    "feature_dir = \"E:/CS541 - Deep Learning/noisy_audio_np\"\n",
    "label_dir = \"E:/CS541 - Deep Learning/clean_audio_np\"\n",
    "\n",
    "# Load all feature and label file paths\n",
    "feature_files = sorted(os.listdir(feature_dir))\n",
    "label_files = sorted(os.listdir(label_dir))\n",
    "\n",
    "assert len(feature_files) == len(label_files), \"Mismatch between feature and label file counts!\"\n",
    "\n",
    "# Model parameters\n",
    "freq_bins, time_bins = 128, 860  # Based on the dimensions of your spectrograms\n",
    "hidden_dim = 64                  # Hidden dimension for spiking neurons\n",
    "num_steps = 10                   # Number of timesteps for spiking neurons\n",
    "num_subbands = 4                 # Number of subbands (adjusted for even division)\n",
    "beta = 0.9                       # Decay parameter for LIF neurons\n",
    "\n",
    "# Initialize the model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = IntegratedModel(freq_bins, time_bins, hidden_dim, num_steps, num_subbands, beta).to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Track losses\n",
    "losses = []\n",
    "\n",
    "# Batch processing\n",
    "for idx in tqdm(range(len(feature_files)), desc=\"Processing files\"):\n",
    "    # Load feature and label\n",
    "    feature_path = os.path.join(feature_dir, feature_files[idx])\n",
    "    label_path = os.path.join(label_dir, label_files[idx])\n",
    "\n",
    "    noisy_tensor = torch.tensor(np.load(feature_path)).unsqueeze(0).to(device)  # Shape: (1, freq_bins, time_bins)\n",
    "    clean_tensor = torch.tensor(np.load(label_path)).unsqueeze(0).to(\"cpu\")   # Shape: (1, freq_bins, time_bins)\n",
    "\n",
    "    clean_time_bins = clean_tensor.shape[2]  # Time bins of the clean tensor\n",
    "\n",
    "    # Forward pass\n",
    "    torch.cuda.empty_cache()\n",
    "    with torch.no_grad():\n",
    "        filtered_output = model(noisy_tensor.to(\"cuda\"), num_steps=10, clean_time_bins=clean_time_bins)\n",
    "        filtered_output = filtered_output.to(\"cpu\")\n",
    "        # print(f\"Filtered output shape: {filtered_output.shape}\")\n",
    "        # print(f\"Clean tensor shape: {clean_tensor.shape}\")\n",
    "\n",
    "    # Compute loss\n",
    "    loss = loss_fn(filtered_output, clean_tensor)\n",
    "    losses.append(loss)\n",
    "    # print(f\"Loss: {loss.item()}\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    # Print shapes for debugging (optional, comment for large datasets)\n",
    "    # print(f\"Processed file {idx + 1}/{len(feature_files)}: Loss = {loss.item()}\")\n",
    "\n",
    "# Print average loss across all files\n",
    "average_loss = sum(losses) / len(losses)\n",
    "print(f\"\\nAverage Loss (MSE) across all files: {average_loss:.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
